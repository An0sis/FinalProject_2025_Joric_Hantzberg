{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d954d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import only necessary libraries\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.sql.functions import explode, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff12fa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading big matrix...\n",
      "Loading small matrix...\n",
      "Loading social network...\n",
      "Loading item features...\n",
      "Loading user features...\n",
      "Loading items' daily features...\n",
      "All data loaded.\n"
     ]
    }
   ],
   "source": [
    "# Set the path to the data directory\n",
    "data_path = \"../data/KuaiRec/data/\"\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading big matrix...\")\n",
    "big_matrix = pd.read_csv(data_path + \"big_matrix.csv\")\n",
    "print(\"Loading small matrix...\")\n",
    "small_matrix = pd.read_csv(data_path + \"small_matrix.csv\")\n",
    "print(\"Loading social network...\")\n",
    "social_network = pd.read_csv(data_path + \"social_network.csv\")\n",
    "social_network[\"friend_list\"] = social_network[\"friend_list\"].map(eval)\n",
    "print(\"Loading item features...\")\n",
    "item_categories = pd.read_csv(data_path + \"item_categories.csv\")\n",
    "item_categories[\"feat\"] = item_categories[\"feat\"].map(eval)\n",
    "print(\"Loading user features...\")\n",
    "user_features = pd.read_csv(data_path + \"user_features.csv\")\n",
    "print(\"Loading items' daily features...\")\n",
    "item_daily_features = pd.read_csv(data_path + \"item_daily_features.csv\")\n",
    "print(\"All data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97487ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the interaction matrix for training\n",
    "interaction_matrix = big_matrix[[\"user_id\", \"video_id\", \"watch_ratio\"]]\n",
    "interaction_matrix = interaction_matrix[interaction_matrix['watch_ratio'] <= 3]\n",
    "interaction_matrix['watch_ratio'] = (\n",
    "    (interaction_matrix['watch_ratio'] - interaction_matrix['watch_ratio'].min()) /\n",
    "    (interaction_matrix['watch_ratio'].max() - interaction_matrix['watch_ratio'].min())\n",
    ")\n",
    "\n",
    "# Prepare the test matrix in the same way\n",
    "test_matrix = small_matrix[[\"user_id\", \"video_id\", \"watch_ratio\"]]\n",
    "test_matrix = test_matrix[test_matrix['watch_ratio'] <= 3]\n",
    "test_matrix['watch_ratio'] = (\n",
    "    (test_matrix['watch_ratio'] - test_matrix['watch_ratio'].min()) /\n",
    "    (test_matrix['watch_ratio'].max() - test_matrix['watch_ratio'].min())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d793bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/17 16:33:51 WARN Utils: Your hostname, Anosis-laptop resolves to a loopback address: 127.0.1.1; using 192.168.1.33 instead (on interface wlp4s0)\n",
      "25/05/17 16:33:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/17 16:33:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Start Spark session with specified memory settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ALS KuaiRec\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Xs32m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Convert pandas DataFrames to Spark DataFrames\n",
    "spark_df = spark.createDataFrame(interaction_matrix)\n",
    "spark_df_test = spark.createDataFrame(test_matrix)\n",
    "\n",
    "# Cast columns to appropriate types for ALS\n",
    "spark_df = spark_df.withColumn(\"userId\", spark_df[\"user_id\"].cast(IntegerType()))\n",
    "spark_df = spark_df.withColumn(\"itemId\", spark_df[\"video_id\"].cast(IntegerType()))\n",
    "spark_df = spark_df.withColumn(\"rating\", spark_df[\"watch_ratio\"].cast(FloatType()))\n",
    "spark_df_test = spark_df_test.withColumn(\"userId\", spark_df_test[\"user_id\"].cast(IntegerType()))\n",
    "spark_df_test = spark_df_test.withColumn(\"itemId\", spark_df_test[\"video_id\"].cast(IntegerType()))\n",
    "spark_df_test = spark_df_test.withColumn(\"rating\", spark_df_test[\"watch_ratio\"].cast(FloatType()))\n",
    "\n",
    "# Repartition and persist for performance\n",
    "spark_df = spark_df.repartition(500, \"userId\").persist()\n",
    "spark_df_test = spark_df_test.repartition(500, \"userId\").persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e449065d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/17 16:36:42 WARN TaskSetManager: Stage 0 contains a task of very large size (12209 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/05/17 16:36:54 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "25/05/17 16:37:32 WARN TaskSetManager: Stage 82 contains a task of very large size (4643 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.1523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 202:(455 + 18) / 500][Stage 238:>  (0 + 0) / 1][Stage 240:>  (0 + 0) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+------+------+----------+----------+\n",
      "|user_id|video_id|        watch_ratio|userId|itemId|    rating|prediction|\n",
      "+-------+--------+-------------------+------+------+----------+----------+\n",
      "|    833|    3631| 0.3598314406113849|   833|  3631|0.35983145| 0.3771991|\n",
      "|    833|    1963| 0.3506944444444444|   833|  1963|0.35069445| 0.3606498|\n",
      "|    833|    9592|0.37574811345303144|   833|  9592|0.37574813|0.40438125|\n",
      "|    833|    5262|  0.417341089192379|   833|  5262|0.41734108|0.42578137|\n",
      "|    833|    1922| 0.3625562689100435|   833|  1922|0.36255628|0.37377772|\n",
      "|    833|    6787|  0.874326906374848|   833|  6787| 0.8743269|0.85503644|\n",
      "|    833|    6788|0.22572653006562207|   833|  6788|0.22572653|0.22418948|\n",
      "|    833|     166| 0.5999404761904762|   833|   166| 0.5999405| 0.5772757|\n",
      "|    833|    6775| 0.2863189591500483|   833|  6775|0.28631896|0.27551708|\n",
      "|    833|     183| 0.5539890710382513|   833|   183|0.55398905|0.58110744|\n",
      "|    833|    1936|0.47394366197183097|   833|  1936|0.47394365|0.47805154|\n",
      "|    833|     171| 0.6373014366179356|   833|   171|0.63730145|0.65507853|\n",
      "|    833|    9571|0.38893428271273495|   833|  9571|0.38893428| 0.4510981|\n",
      "|    833|    5222| 0.4010975609756097|   833|  5222|0.40109757|0.44462046|\n",
      "|    833|    5253|0.07682922964793863|   833|  5253|0.07682923|0.08555154|\n",
      "|    833|    5288|0.22460850111856823|   833|  5288| 0.2246085|0.24641906|\n",
      "|    833|     169|0.34039793863690654|   833|   169|0.34039792|0.36880797|\n",
      "|    833|    8195| 0.5838596491228071|   833|  8195| 0.5838596| 0.5766398|\n",
      "|    833|    6767|0.31928358964622494|   833|  6767|0.31928357| 0.3819827|\n",
      "|    833|    6801|0.47483052069811055|   833|  6801| 0.4748305|  0.466845|\n",
      "+-------+--------+-------------------+------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train ALS model\n",
    "als = ALS(\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"itemId\",\n",
    "    ratingCol=\"rating\",\n",
    "    rank=50,\n",
    "    maxIter=15,\n",
    "    regParam=0.001,\n",
    "    implicitPrefs=False,\n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "model = als.fit(spark_df)\n",
    "\n",
    "# Evaluate model on test set\n",
    "predictions = model.transform(spark_df_test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "predictions.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "670627ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate at 10: 0.0468\n",
      "      user_id                                           video_id\n",
      "0          14  [601, 2427, 4051, 3105, 5753, 738, 2292, 4644,...\n",
      "1          19  [154, 5464, 2629, 8366, 9178, 4592, 4092, 1305...\n",
      "2          21  [5205, 8488, 2941, 9804, 5412, 9815, 10206, 67...\n",
      "3          23  [2302, 3738, 522, 9611, 8802, 2283, 3130, 791,...\n",
      "4          24  [9886, 3720, 8431, 6266, 1415, 8596, 7135, 304...\n",
      "...       ...                                                ...\n",
      "1406     7142  [4123, 1305, 6787, 351, 10206, 619, 2894, 5525...\n",
      "1407     7147  [314, 9721, 9613, 7752, 9815, 8298, 7564, 6787...\n",
      "1408     7153  [1922, 2478, 7820, 211, 9998, 171, 7844, 3742,...\n",
      "1409     7159  [4282, 2223, 4123, 9721, 4858, 395, 6879, 7794...\n",
      "1410     7162  [4932, 5276, 9936, 1396, 217, 8629, 1288, 8670...\n",
      "\n",
      "[1411 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      user_id                                           video_id\n",
      "0          14  [4256, 7755, 5284, 9699, 6596, 868, 6418, 6358...\n",
      "1          19  [1087, 908, 3556, 6214, 6538, 6596, 7933, 1024...\n",
      "2          21  [5284, 4546, 6596, 5488, 8797, 2992, 5430, 843...\n",
      "3          23  [6547, 6186, 2872, 3367, 6816, 8052, 6389, 843...\n",
      "4          24  [421, 5107, 1654, 10507, 7424, 5110, 946, 9703...\n",
      "...       ...                                                ...\n",
      "1406     7142  [9637, 6625, 3453, 7644, 908, 6200, 3367, 9309...\n",
      "1407     7147  [3001, 6596, 1049, 9079, 7933, 2982, 1087, 689...\n",
      "1408     7153  [1163, 3434, 1781, 902, 1559, 9698, 7968, 1054...\n",
      "1409     7159  [9426, 6483, 6360, 5056, 6625, 3305, 9189, 104...\n",
      "1410     7162  [6402, 8006, 1462, 5927, 8434, 10132, 6642, 70...\n",
      "\n",
      "[1411 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to get top-k actual items per user from test set\n",
    "def get_top_k_df(k=10):\n",
    "    df_sorted = test_matrix.sort_values(['user_id', 'watch_ratio'], ascending=[True, False])\n",
    "    df_sorted = df_sorted.groupby('user_id').head(k)\n",
    "    return df_sorted.groupby('user_id')['video_id'].apply(list).reset_index()\n",
    "\n",
    "# Function to get top-k predicted items per user using ALS model\n",
    "def format_top_k_predictions(k=10):\n",
    "    test_user_ids = spark_df_test.select(\"userId\").distinct()\n",
    "    # Recommend top-k items for each user in the test set\n",
    "    user_recommendations = model.recommendForUserSubset(test_user_ids, k)\n",
    "    # Flatten recommendations\n",
    "    exploded_recs = user_recommendations.select(\n",
    "        col(\"userId\"),\n",
    "        explode(col(\"recommendations\")).alias(\"recommendation\")\n",
    "    )\n",
    "    final_recs = exploded_recs.select(\n",
    "        col(\"userId\").alias(\"user_id\"),\n",
    "        col(\"recommendation.itemId\").alias(\"video_id\"),\n",
    "        col(\"recommendation.rating\").alias(\"watch_ratio\")\n",
    "    )\n",
    "    df = final_recs.toPandas()\n",
    "    df = df.groupby(\"user_id\").head(k)\n",
    "    return df.groupby(\"user_id\")[\"video_id\"].apply(list).reset_index()\n",
    "\n",
    "# Function to compute hit rate at k\n",
    "def hit_rate_at_k(k=10):\n",
    "    df = get_top_k_df(k)\n",
    "    df_recommendations = format_top_k_predictions(k)\n",
    "    # Merge actual and predicted top-k lists\n",
    "    merged_df = pd.merge(df, df_recommendations, on='user_id', suffixes=('_actual', '_predicted'))\n",
    "    # Calculate hit rate for each user\n",
    "    hit_rate = merged_df.apply(lambda row: len(set(row['video_id_actual']) & set(row['video_id_predicted'])), axis=1)\n",
    "    return hit_rate.mean()\n",
    "\n",
    "# Print hit rate and sample outputs\n",
    "print(f\"Hit Rate at {10}: {hit_rate_at_k(10):.4f}\")\n",
    "print(get_top_k_df(10))\n",
    "print(format_top_k_predictions(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
